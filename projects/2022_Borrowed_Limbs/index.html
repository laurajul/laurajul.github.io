<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Borrowed Limbs | Laura Wagner </title> <meta name="author" content="Laura Wagner"> <meta name="description" content="Text-to-image short film"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a26b1b0e2d9c9dd2e5ed17c987a37731"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?9dbe0c76bacd463500b79ed6a86e27ac"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://laurajul.github.io/projects/2022_Borrowed_Limbs/"> <script src="/assets/js/theme.js?7dd3b429d8d7795e439794ee2185f3bd"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Laura</span> Wagner </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">laura wagner </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/art/">art </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5" role="main"> <div class="post">   <header class="post-header">     <h1 class="post-title">Borrowed Limbs</h1>     <p class="post-description">Text-to-image short film</p>   </header>   <article>     <h4 id="2021---2022">2021 - 2022</h4> <h3 id="an-experimental-short-film">An Experimental Short Film</h3> <p>The experimental short film “Borrowed Limbs”, created in collaboration with <a href="https://lisamarleen.de/" rel="external nofollow noopener" target="_blank">Lisa-Marleen Mantel</a>, emphasizes the mediating role speculative design has in the face of disruptive technological advancements, as it allows for the exploration and simulation of ethical issues. The short film is centered around an AI main character who uses the human body as a sensing device to gain a physical understanding of their surroundings. Influenced by posthuman thinkers, the work aims to provoke a discussion about Cartesian notions of disembodied intelligence.</p> <p>The work combines recent developments in philosophy and cognitive science with state-of-the-art technologies and artistic practice: Main part of the film was produced with a machine-learning-backed process called “CLIP guided diffusion”. This method enables image synthesis based on language prompts. Prompts are carefully refined in a human-machine communication loop. In a way, the practical processes involved in producing the film reflect the speculative idea of an ongoing dialogue found in the narrative. The work was created as part of the Master Research at KISD – Cologne International School of Design (Faculty of TH Köln).</p> <p>The short film “Borrowed Limbs” was created using a <strong>artistic appropriation of AI tools</strong>, with a focus on exploring the symbiotic relationship between humans and AI, and subverting common science fiction tropes about AI and cyborgs. The production intentionally avoided hackneyed narratives, particularly those romanticizing or sexualizing female cyborgs or depicting evil gynoids.</p> <hr> <h3 id="production-process">Production Process</h3> <p>The core of the film’s footage was generated through a novel process called CLIP-guided image synthesis<sup id="fnref:crowson"><a href="#fn:crowson" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> as pioneered by Katherine Crowson. This method involves image synthesis based on language prompts, which were meticulously refined in a human-machine feedback loop. This iterative process involved prompting the machine, evaluating the results, and then refining subsequent prompts to achieve the desired visual outcome. This creative process is mirrored to the AI’s journey (the film’s protagonist) to becoming an embodied, complete agent within the film’s narrative.</p> <p>A significant aspect of the production involved decoupling the AI from a physical body, transmitting it as disembodied data streams. This was visually represented through different visual aesthetics that conveyed various states of “connectedness” between the AI and its human “device” (a human body utilized by the AI). The film aims to provide insight into an AI’s training iteration, with the constant loop of the AI becoming an embodied agent mirroring the film’s production.</p> <p>To visualize the AI’s hypothetical <strong>birth of consciousness</strong>, the filmmakers experimented with the visual output of the very first training iterations of StyleGAN Networks. They specifically used StyleGAN3 by NVlabs, a Generative Adversarial Network (GAN) where a generator creates images and a discriminator evaluates them. The initial noise distribution of a neural network was used to represent the “incoherence” before the AI perceived the world, speculating that noise might be a “frightening sight” to a machine.</p> <p>For scenes depicting the “human device,” a StyleGAN3 model was trained on approximately 2000 portrait shots of a single human model. This allowed for the exploration of latent space interpolations, where the StyleGAN, trained on one person, could interpolate between movements and facial expressions in an “unnatural manner” while still maintaining natural individual poses and expressions. The aim was to show the human representation through “machine eyes.”</p> <p>The process of visualizing the birth of consciousness was specifically achieved through truncation interpolation, a method published by <a href="https://dvschultz.github.io/design/index.html" rel="external nofollow noopener" target="_blank">Derrick Schultz</a>. Truncation, a parameter for StyleGAN inference, allows the generating algorithm to “wander off” into less likely results in the latent space until it devolves into incoherent nothingness the perlin noise distribution of the inital noise.</p> <p><strong>Prompt design</strong> was crucial, with careful attention to overall composition and color scheme. Objects and color contrasts were used to create important shapes and image compositions. For instance, the “attachment” of the AI to the human model was achieved using a bathing cap and woolen balls, with skin tone faded to match the bathing cap for a “grown-to-the-head” look. Style frames were created to define the look of scenes and integrate prompt design, showing different stages of connection: detachment, symbiotic fusion, attachment, and inner view, each with specific prompts and parameters.</p> <hr> <h3 id="technical-details">Technical Details</h3> <ul> <li> <strong>Core Image Generation Process:</strong> CLIP-guided diffusion</li> <li> <strong>AI Models Used:</strong> <ul> <li> <strong>CLIP</strong> (for guiding image generations)</li> <li> <strong>VQGAN</strong> (for producing high visual quality outputs with CLIP guidance)</li> <li> <strong>StyleGAN3 by NVlabs</strong> (for visualizing the birth of consciousness and generating footage of the “human device”)</li> </ul> </li> <li> <strong>Training Data for StyleGAN3:</strong> Approximately 2000 portrait shots of a human model, maintaining constant image composition.</li> <li> <strong>Training Duration for StyleGAN3:</strong> Approximately four days.</li> <li> <strong>Image Generation Technique for Birth of Consciousness:</strong> Truncation-interpolation (using truncation values in StyleGAN inference).</li> <li> <strong>Software/Libraries:</strong> <ul> <li> <code class="language-plaintext highlighter-rouge">pytti 5 beta.ipynb</code> (mentioned for text prompt usage)</li> <li> <code class="language-plaintext highlighter-rouge">NVlabs/Stylegan3</code> (for StyleGAN3 infrastructure)</li> <li> <code class="language-plaintext highlighter-rouge">StyleGAN-3-fun repository</code> (an adaptation of StyleGAN3 by Diego Porres, used for discriminator synthesis experimentation)</li> </ul> </li> </ul> <hr> <h2>Watch the film</h2> <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;"> <iframe src="https://player.vimeo.com/video/682432232" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" frameborder="0" allow="autoplay; fullscreen; picture-in-picture" allowfullscreen=""> </iframe> </div> <div style="margin: 3rem 0;"> <hr style="border: none; border-top: 1px solid #ccc;"> </div> <h2>List of screenings</h2> <table> <thead> <tr> <th>Date</th> <th>Event</th> <th>Location</th> </tr> </thead> <tbody> <tr> <td>August 2022</td> <td><a href="https://ars.electronica.art/planetb/en/welcome-to-planetb/" target="_blank" rel="noopener">Ars Electronica</a></td> <td>Linz, Austria</td> </tr> <tr> <td>November 2022</td> <td>Museumsnacht Köln – <em>States of Motion</em> </td> <td>Cologne, Germany</td> </tr> <tr> <td>August 2022</td> <td><a href="https://site.gardening.nu/en/event/gardening" target="_blank" rel="noopener">Gardening Amelisweerd</a></td> <td>Utrecht, the Netherlands</td> </tr> <tr> <td>August 2022</td> <td><a href="https://example.com/phantom-horizon" target="_blank" rel="noopener">Phantom Horizon</a></td> <td>Künstlerhaus Bethanien</td> </tr> <tr> <td>August 2022</td> <td>KISD Parcours</td> <td>Cologne, Germany</td> </tr> </tbody> </table> <hr> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:crowson"> <p>Katherine Crowson et al., “VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance,” arXiv, last revised September 4, 2022, arXiv:2204.08583, https://doi.org/10.48550/arXiv.2204.08583. <a href="#fnref:crowson" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div>   </article>     </div>   </div> <script src="/assets/js/jquery.min.js"></script> <script>
    var jekyllBaseurl = "";
</script> <script src="/assets/js/svg_sliders.js"></script> <script src="/assets/js/reveal.js"></script> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Laura Wagner. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?3e7054dc4d3e3dd8f0731a48453e618e"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?2b177038d0885a876f47a3df6474238a" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?982c80efc910ea9a6203400dcbd0a3af"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?3768d5c2a8ada32872960d47d757c2ac"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>